{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial Emotion Recognition using CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztky6L9E-lnP",
        "colab_type": "text"
      },
      "source": [
        "# **Facial Emotion Recognition**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Dataset - https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHrzqEejOjR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_author_ = \" Prudhvi_GNV \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CULEspsFALh",
        "colab_type": "text"
      },
      "source": [
        "**kaggle --account--Api**\n",
        "API's are used to access data from kaggle\n",
        "1. kaggle a/c -> my profile -> create api token\n",
        "2. In colabs ..while uploading api file... go to /content\n",
        "3. make .kaggle directory in /root \n",
        "4. copy kaggle api from /content to /root/.kaggle\n",
        "5. kaggle cmd(find in kaggle dataset page-> data) to download\n",
        "6. unzip the downloaded dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbi7hj_R8_yD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "821b2a4d-cf9c-4511-db4e-81d97a403842"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt4jf1OW_6gf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19a22d24-44f2-4198-a522-706ec250a91f"
      },
      "source": [
        "cd /root"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEY-3HG5AHFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir .kaggle               # .kaggle not /.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNO-HFFq85a0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4247e10-6aab-422b-a1bc-2dc90ed1a465"
      },
      "source": [
        "cd /root/.kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqSXEsJ29HP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/kaggle.json ~/.kaggle \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN48VBLQpQb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca46b866-1ee6-4b15-d5e4-b8c04257c39f"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.kaggle'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YSkULw_pT0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80e39c48-8bb8-49ab-c1cd-857aa1cca10a"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vlkz9IP9otV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f83ebad3-f048-47f0-d3ee-9b3b8f18a37b"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEkxCltOmE5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "30fc2762-aa92-452e-f9fd-4c5525d13f27"
      },
      "source": [
        "# if ..looks like outdated api ..server version ERROR occurs\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/bb20f9b9e24f9a6250f95a432f8d9a7d745f8d24039d7a5a6eaadb7783ba/kaggle-1.5.6.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.6-cp36-none-any.whl size=72859 sha256=0689620788124ce19b2ecf97618b3447d3c4da0549599431734918529a9826aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/4e/e8/bb28d035162fb8f17f8ca5d42c3230e284c6aa565b42b72674\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.6\n",
            "    Uninstalling kaggle-1.5.6:\n",
            "      Successfully uninstalled kaggle-1.5.6\n",
            "Successfully installed kaggle-1.5.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytfZi3nJmilT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#if ..only read permisions ..ERROR\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTPl804d77nk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e3bf67fe-e747-4999-8321-ecfc11f9840d"
      },
      "source": [
        "# if 403 forbidden ERROR ... then accept the rules in kaggle dataset page\n",
        "\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /root/.kaggle\n",
            " 96% 273M/285M [00:05<00:00, 38.1MB/s]\n",
            "100% 285M/285M [00:05<00:00, 50.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grCSTKE-_UG3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c23805f9-096b-42b4-aa2f-ecf05809922e"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEobtx6O9qvT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d2252ea3-ed19-49c4-86f5-4a248f8e9082"
      },
      "source": [
        "!unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiYFWW7_vMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ce44cebc-d1e3-4f07-9b65-c38ffc0d2b4f"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "example_submission.csv\n",
            "fer2013.tar.gz\n",
            "icml_face_data.csv\n",
            "kaggle.json\n",
            "test.csv\n",
            "train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQgMssSBwypP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf fer2013.tar.gz   # x= extract , f= to create archive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiO3ojiRw4wW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "e0e40a5c-a7b9-4b91-9082-966b73da2aa8"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "example_submission.csv\n",
            "\u001b[0m\u001b[01;34mfer2013\u001b[0m/\n",
            "fer2013.tar.gz\n",
            "icml_face_data.csv\n",
            "kaggle.json\n",
            "test.csv\n",
            "train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIq55w1FF5fQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04b0e543-2e6f-4f3c-f969-eba890e9be24"
      },
      "source": [
        "cd fer2013"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.kaggle/fer2013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaVtewzUF-zA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a534b5ca-77cd-488c-cfd2-2c6e1af274cc"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fer2013.bib  fer2013.csv  README\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YeU4HHZ_bS4",
        "colab_type": "text"
      },
      "source": [
        "## **Dataset**\n",
        "* The data consists of 48x48 pixel grayscale images of faces. \n",
        " \n",
        "* The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
        "* Dataset contains two columns, \"emotion\" and \"pixels\".\n",
        "* The \"emotion\" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image. \n",
        "* The \"pixels\" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. test.csv contains only the \"pixels\" column and your task is to predict the emotion column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zu89RTzBMty",
        "colab_type": "text"
      },
      "source": [
        "# Data Visualization\n",
        "\n",
        "* powerful visualization tools => matplotlib, seaborn\n",
        "      1. matplotlib works on pandas dataframe\n",
        "      2.  matplotlib => basic ploting =>bars,pie,scatter, lines etc.. => used for MATLAB like graphs\n",
        "      3. seaborn => statistical ploting => depends on matplotlib...high level => provides default templates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcfiDv03-cGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b97002a4-fa58-4cf7-d3d0-8e4b7394ef20"
      },
      "source": [
        "\"\"\" IMPORT ALL DEPENDENCIES\"\"\"\n",
        "\n",
        "\n",
        "# NumPy for numerical computing\n",
        "import numpy as np\n",
        "\n",
        "# Pandas for DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "# Matplotlib for visualization\n",
        "from matplotlib import pyplot as plt\n",
        "# display plots in the notebook\n",
        "%matplotlib inline\n",
        "# import color maps\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Seaborn for easier visualization\n",
        "import seaborn as sns"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTJESy7QA-5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "624d7579-649e-4869-f3e8-b9ef0ffca898"
      },
      "source": [
        "df = pd.read_csv(\"/root/.kaggle/fer2013/fer2013.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbnhU1whADCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "4c45e608-8b5b-4a63-a0ab-c75f37ec8b69"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35882</th>\n",
              "      <td>6</td>\n",
              "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35883</th>\n",
              "      <td>3</td>\n",
              "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35884</th>\n",
              "      <td>0</td>\n",
              "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35885</th>\n",
              "      <td>3</td>\n",
              "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35886</th>\n",
              "      <td>2</td>\n",
              "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>35887 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion                                             pixels        Usage\n",
              "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
              "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
              "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
              "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
              "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
              "...        ...                                                ...          ...\n",
              "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
              "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
              "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
              "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
              "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
              "\n",
              "[35887 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prR_SJxiBA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df4086bb-a7fb-4fcb-fd1a-755a89841b01"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35887, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k39FElpEMVju",
        "colab_type": "text"
      },
      "source": [
        "## **Plotting emotion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZidMumXvBzpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5dca35ad-5622-495b-ed70-04b02154e64d"
      },
      "source": [
        "\"\"\" matplotlib => to define size , sns => to use counterplot \"\"\"\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "sns.countplot(x='emotion', data=df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcd333b8d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qEMmC3ILEtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "927affad-d1b7-4dcc-f584-7ebc7886d6cf"
      },
      "source": [
        "df['emotion'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wah5ra4rMB0O",
        "colab_type": "text"
      },
      "source": [
        "* where ** 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iblqx4zrSuUY",
        "colab_type": "text"
      },
      "source": [
        "### Observation\n",
        "* for Digust we have about 547 images only very less. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4G1tEVgMnl0",
        "colab_type": "text"
      },
      "source": [
        "## **Plotting Usage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL5-9TQJMAbi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "4ab89dbf-dc7d-4b0f-8961-d5c3d50848ea"
      },
      "source": [
        "plt.figure(figsize=(9,4))\n",
        "sns.countplot(x='Usage', data=df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcd332b23c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTzE2z-2Lpsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ff1d16a4-c894-4bbf-e5b7-ea366f344598"
      },
      "source": [
        "df['Usage'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Training       28709\n",
              "PrivateTest     3589\n",
              "PublicTest      3589\n",
              "Name: Usage, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Xc1kb96J_I",
        "colab_type": "text"
      },
      "source": [
        " **Intuition**\n",
        "\n",
        "* % matplotlib inline ==> magic cmd to setup IPython to display and store figures in notebooks\n",
        "* dataframe ==>pd.read_csv() , .head(), .tail(), .shape\n",
        "* count ==>  sns.countplot(x=,df) -> for graphs  ; df[col].value_counts() -> to display count values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLpNo_Wi78C3",
        "colab_type": "text"
      },
      "source": [
        "# PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OshjJPMx95JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "image_size=(48,48)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRs4m6UvM7AO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pixels = df['pixels'].tolist() # Converting the relevant column element into a list for each row\n",
        "width, height = 48, 48\n",
        "faces = []\n",
        "\n",
        "for pixel_sequence in pixels:\n",
        "  face = [int(pixel) for pixel in pixel_sequence.split(' ')] # Splitting the string by space character as a list\n",
        "  face = np.asarray(face).reshape(width, height) #converting the list to numpy array in size of 48*48\n",
        "  face = cv2.resize(face.astype('uint8'),image_size) #resize the image to have 48 cols (width) and 48 rows (height)\n",
        "  faces.append(face.astype('float32')) #makes the list of each images of 48*48 and their pixels in numpyarray form\n",
        "  \n",
        "faces = np.asarray(faces) #converting the list into numpy array\n",
        "faces = np.expand_dims(faces, -1) #Expand the shape of an array -1=last dimension => means color space\n",
        "emotions = pd.get_dummies(df['emotion']).to_numpy() #doing the one hot encoding type on emotions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPsO_9sn77EB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "e8f48f97-29d1-47dd-ed72-f41a24d1efb4"
      },
      "source": [
        "print(faces[0]) #Pixels after preprocessing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 70.]\n",
            "  [ 80.]\n",
            "  [ 82.]\n",
            "  ...\n",
            "  [ 52.]\n",
            "  [ 43.]\n",
            "  [ 41.]]\n",
            "\n",
            " [[ 65.]\n",
            "  [ 61.]\n",
            "  [ 58.]\n",
            "  ...\n",
            "  [ 56.]\n",
            "  [ 52.]\n",
            "  [ 44.]]\n",
            "\n",
            " [[ 50.]\n",
            "  [ 43.]\n",
            "  [ 54.]\n",
            "  ...\n",
            "  [ 49.]\n",
            "  [ 56.]\n",
            "  [ 47.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 91.]\n",
            "  [ 65.]\n",
            "  [ 42.]\n",
            "  ...\n",
            "  [ 72.]\n",
            "  [ 56.]\n",
            "  [ 43.]]\n",
            "\n",
            " [[ 77.]\n",
            "  [ 82.]\n",
            "  [ 79.]\n",
            "  ...\n",
            "  [105.]\n",
            "  [ 70.]\n",
            "  [ 46.]]\n",
            "\n",
            " [[ 77.]\n",
            "  [ 72.]\n",
            "  [ 84.]\n",
            "  ...\n",
            "  [106.]\n",
            "  [109.]\n",
            "  [ 82.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2JENcOV_nXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9d946358-ded7-4489-c937-28b8208204de"
      },
      "source": [
        "print(faces.shape)\n",
        "print(faces[0].ndim)\n",
        "print(type(faces))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35887, 48, 48, 1)\n",
            "3\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeQMsBoR-Y4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d68aeb8b-f592-48aa-dd0d-8018b88a1572"
      },
      "source": [
        "print(emotions[0]) #Emotion after preprocessing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOO-ZRSf-hgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "25f89382-4486-4098-a635-f66ae1dfd196"
      },
      "source": [
        "print(emotions.shape)\n",
        "print(emotions.ndim)\n",
        "print(type(emotions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35887, 7)\n",
            "2\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj9OqVepeZ2E",
        "colab_type": "text"
      },
      "source": [
        "**Intuition**\n",
        "* Pandas series(or col of dataframe) to list => tolist() =>gives list of string of values->each string is a image\n",
        "* split each string(image) in list and cvt them into list of list of integers\n",
        "* convert list into numpy array =>np.asarray(list)\n",
        "* numpy array has reshape(width,height) \n",
        "* cv2 has resize(nparray, (size)) -->cvt image into uint8(unsigned 8 bit integer) -> has range [0,255]\n",
        "  1. uint16 => [0,65535]\n",
        "  2. uint32 => 32 bit or 4 bytes => [0, 2^32]\n",
        "  3. float => [-1,1] or [0,1]\n",
        "  4. int8 => [-128, 127]\n",
        "  5. int =>  [-2^31 , 2^31 -1] \n",
        "* images => ntg but numpy array\n",
        "* openCV has 3 dimensions => width,height, color => color space = BGR ; dtype => by default uint8\n",
        "* reshape returns a copy [ doesn't change values] whereas resize changes the values[not return a copy]\n",
        "* np.expand_dims => inserts new axis => ntg but boxes(just think for convenience) => to increase dimensions => to expand shape\n",
        "  1. axis= 0 -> box to all values ; axis =1 -> boxes to each individual values\n",
        "  2. example , (2,1,2) =>[ [[[],[]]],[[[],[]]] ] => list has 2 boxes - each box has 1 box - inside that each 1 box has 2 boxes --> in general box is a list or value\n",
        "* pd.get_dummies => convert pd series into dummy coded dataframes or indicator variables. => categorical var into indicators vars. => s.no(row names) * values(cols names) - 0,1 values.\n",
        "* numpy.asmatrix(data,dtype) => interpret input as a matrix => doesn't return a copy of matrix if given input is a matrix.\n",
        "  1. In list[0][1] =>In numpy[0,1] -> means In first box 2nd ele.\n",
        "*pd.df.to_numpy() => converts pandas dataframe into numpy array -> ndarray - usally 2 dim ->\n",
        "  1. dataframe -> some kind of tabular format with some indexes. ; numpy array -> some kind of lists of lists\n",
        "  2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL78P7u0I1Hp",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28T5zBUCUn4j",
        "colab_type": "text"
      },
      "source": [
        "## Scaling the pixels between -1 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2MJ1pnPJCyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = faces.astype('float32')\n",
        "x = x / 255.0 #Dividing the pixels by 255 for normalization  => range(0,1)\n",
        "\n",
        "# Scaling the pixels value in range(-1,1)\n",
        "x = x - 0.5\n",
        "x = x * 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foWH2FX3JErZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "6beda3b8-2474-47b7-e1ab-e9ffbe109903"
      },
      "source": [
        "print(x[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[-0.45098037]\n",
            "  [-0.372549  ]\n",
            "  [-0.35686272]\n",
            "  ...\n",
            "  [-0.5921569 ]\n",
            "  [-0.6627451 ]\n",
            "  [-0.6784314 ]]\n",
            "\n",
            " [[-0.49019605]\n",
            "  [-0.52156866]\n",
            "  [-0.54509807]\n",
            "  ...\n",
            "  [-0.56078434]\n",
            "  [-0.5921569 ]\n",
            "  [-0.654902  ]]\n",
            "\n",
            " [[-0.60784316]\n",
            "  [-0.6627451 ]\n",
            "  [-0.5764706 ]\n",
            "  ...\n",
            "  [-0.6156863 ]\n",
            "  [-0.56078434]\n",
            "  [-0.6313726 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.2862745 ]\n",
            "  [-0.49019605]\n",
            "  [-0.67058825]\n",
            "  ...\n",
            "  [-0.4352941 ]\n",
            "  [-0.56078434]\n",
            "  [-0.6627451 ]]\n",
            "\n",
            " [[-0.3960784 ]\n",
            "  [-0.35686272]\n",
            "  [-0.38039213]\n",
            "  ...\n",
            "  [-0.17647058]\n",
            "  [-0.45098037]\n",
            "  [-0.6392157 ]]\n",
            "\n",
            " [[-0.3960784 ]\n",
            "  [-0.4352941 ]\n",
            "  [-0.34117645]\n",
            "  ...\n",
            "  [-0.16862744]\n",
            "  [-0.14509803]\n",
            "  [-0.35686272]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_QhWHs_JGMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7af78ebf-4fbb-432d-ddeb-f784d0eea509"
      },
      "source": [
        "type(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F5u4B83NKFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "8804c429-f539-486f-e01d-0afa62f7a319"
      },
      "source": [
        "plt.plot(x[0,0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euq0pES3Ufv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c2a1ccc-051e-4ce0-fb47-2c4fbad5543f"
      },
      "source": [
        "print(x.min(),x.max()) # we can observe that pixels are scale"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZARK_oB_IuF",
        "colab_type": "text"
      },
      "source": [
        "## Splitting the dataset into train & validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oIyb2zMU2sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_samples, num_classes = emotions.shape\n",
        "\n",
        "num_samples = len(x)\n",
        "num_train_samples = int((1 - 0.2)*num_samples)\n",
        "\n",
        "# Traning data\n",
        "train_x = x[:num_train_samples]\n",
        "train_y = emotions[:num_train_samples]\n",
        "\n",
        "# Validation data\n",
        "val_x = x[num_train_samples:]\n",
        "val_y = emotions[num_train_samples:]\n",
        "\n",
        "train_data = (train_x, train_y)\n",
        "val_data = (val_x, val_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBtesONJWvDY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "59b1c001-692f-4e86-f641-a01be111402f"
      },
      "source": [
        "print('Training Pixels',train_x.shape)  # ==> 4 dims -  no of images , width , height , color\n",
        "print('Training labels',train_y.shape)\n",
        "\n",
        "print('Validation Pixels',val_x.shape)\n",
        "print('Validation labels',val_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Pixels (28709, 48, 48, 1)\n",
            "Training labels (28709, 7)\n",
            "Validation Pixels (7178, 48, 48, 1)\n",
            "Validation labels (7178, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3u-V2UMmKTI",
        "colab_type": "text"
      },
      "source": [
        "* shape => returns no of samples , classes ; len => no of samples\n",
        "* training -> x = x[: 0.8*no_of_samples] , y = y[: 0.8*no_of_samples]     ---> take the benefit of slicing\n",
        "* validation -> x = x[ 0.8*no_of_samples  : ] , y = y[ 0.8*no_of_samples  : ]\n",
        "* data = (x,y)\n",
        "\n",
        "* take benefit of type(), print() --> for checking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyZvjJKTAMpx",
        "colab_type": "text"
      },
      "source": [
        "# Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKpD2mbkD3V0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the libaray to built the model\n",
        "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
        "from keras.layers import AveragePooling2D, BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import SeparableConv2D\n",
        "from keras import layers\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f_8pLj1p85S",
        "colab_type": "text"
      },
      "source": [
        "* keras is a deep learning library => powerful easy to use library => high level API for tensorflow, Theano => wrapper\n",
        "* minimalistic , modular approach\n",
        "* obviously,oversimplification ->abstract representations of neural networks\n",
        "* import \n",
        "  1. Sequential => linear stack of NN layers -> feed forward cnn\n",
        "  2. layers -> for almost any nn\n",
        "  3. CNN layers -> convolution2D , maxpooling2D\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7wHibYv2wF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "628f6660-4454-4595-92c6-8dc51129aebc"
      },
      "source": [
        "\"\"\"\n",
        "* keras.__version__\n",
        "* pip install --upgrade keras\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\n* keras.__version__\\n* pip install --upgrade keras\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyPOvVbXtSUi",
        "colab_type": "text"
      },
      "source": [
        "* Deep learning led to advances in computer vision\n",
        "* Neural networks learn more complex features from input image.\n",
        "  1. input layer => feeding input\n",
        "  2. first hidden layer =>  only learn local edge patterns\n",
        "  3. each subsequent layer => filters => learns more complex representations\n",
        "  4. classify\n",
        "\n",
        "* CNN => reduce no of parameters that need to be tuned => handle high dimensionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lni4RvfCDtgD",
        "colab_type": "text"
      },
      "source": [
        "# 1) Simpler CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h71E2cTFWOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(48, 48, 1)\n",
        "num_classes = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgnHh0urDr7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Building up Model Architecture \"\"\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same',\n",
        "                            name='image_array', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Convolution2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
        "\n",
        "\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=num_classes, kernel_size=(3, 3), padding='same'))\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Activation('softmax',name='predictions'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOB8szvd_tfd",
        "colab_type": "text"
      },
      "source": [
        " # **Intuition**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* **Convolution** => matrix multiplication with filter -> feature detector\n",
        "  * Convolution2d => for filtering windows of 2 dimensional input -> if 1st layer = input_shape\n",
        "* filters = windows ; 2 dim filter => sliding window\n",
        "* sliding window slides over each channel and summarising the features.\n",
        "*  **batch normalisation** => used to stabilise perhaps accelerate learning process => standardise layer inputs => by applying transformation that maintains mean activation close to 0 and activation standard deviation(sq root of variance ---> how far) close to 1.\n",
        "  1. normalisation => process tend to follow bell shape curve known as normal distribution\n",
        "  2. backpropagation => updated layer by layer backward from output to the input using estimate of error that assumes weights in the layer prior to the current layer are fixed.\n",
        "  3. gradient tells how to update each parameter under the assumption that other layers do not change. \n",
        "  4. all layers changes during an update --> this update procedure leads to forever chasing a moving target.\n",
        "  5. batch normalisation => technique to coordinate the update  of multiple layers in the model => reparametrization of network\n",
        "  6. its about standardize the mean and variance of each unit in normal dist\n",
        "  7. its all about standardixeing inputs to layers for each mini batch\n",
        "\n",
        "* **Activation(ReLu)**: activation layer( non linear layer) \n",
        "  1. convention is to apply after conv layer\n",
        "  2. to introduce non linearity to a system that has computed linear operations in conv\n",
        "  3. Rectified linear unit => widely used  than non linear functions(sigmoid, tanh) for its fast training with out accuracy. => max(0,x)\n",
        "  4. Relu also alleviates vanishing gradient (lower layers of network trains very slowly because the gradient decreases slowly throught layers.\n",
        "  5. without these non linear functions(activation functions) , the network would be a large linear classifier that could simplified by multiplying weight matrices(accounting for bais) . It wouldn't do anything interesting such as image classification etc..\n",
        "* **Pooling** => max, avg ,globalmax, globalavg\n",
        "  1. conv > activation > pooling\n",
        "  2.to reduce dimensions of feature map => reduces parameters to learn and amt of computations\n",
        "  3. it further summarizes the feature map  instead of precisely positioned features generated by conv layer. This makes model more robust to variations in the position of features in image\n",
        "  4.when network wants to detect higher level features from low level building blocks (detecting corners from edges) . we dont need a rigid about exact position => we need translational invariance at the feature level . so insert pooling\n",
        "  5. overcomes the problem of sensitive to the location of the features.\n",
        "  6. local translation invariance\n",
        "\n",
        "*  **Dropout** => regularization technique\n",
        "  1.neurons are randomly dropped while training\n",
        "  2. this effect makes network less sensitive to thespecific weights of neurons \n",
        "  3. better generalization - less overfit\n",
        "\n",
        "\n",
        "---\n",
        "**Functions:**\n",
        "* convolution2d => no of filters or kernels , kernal size , padding => same(zero padding) , valid (no padding) for input ; stride = step or movement of kernel. ; if 1st layer -> input_shape\n",
        "* BatchNormalisation => no args \n",
        "* Activation => func name\n",
        "* AveragePooling2D => 2D means 2 dimensional feature map ; kernel size , padding\n",
        "* Dropout => % of drop of neurons \n",
        "* AT output , convolution2d --> filters = no of classes ; activation --> softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1DSW_zoN1hg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bf2c2fd-78fe-44c1-8822-66a5bb757216"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image_array (Conv2D)         (None, 48, 48, 16)        800       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 16)        12560     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 48, 48, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 32)        12832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 12, 12, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 12, 12, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 6, 6, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_4 (Average (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 3, 3, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 3, 3, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 7)           16135     \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 7)                 0         \n",
            "_________________________________________________________________\n",
            "predictions (Activation)     (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 642,935\n",
            "Trainable params: 641,463\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS2a73LMMXsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "batch_size = 32 #Number of samples per gradient update\n",
        "num_epochs = 200 # Number of epochs to train the model.\n",
        "#input_shape = (64, 64, 1)\n",
        "verbose = 1 #per epohs  progress bar\n",
        "num_classes = 7 \n",
        "patience = 50\n",
        "base_path = 'drive/Colab Notebooks/emotion/simplecnn/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZGzV6XKVkmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIB613fzVrlK",
        "colab_type": "text"
      },
      "source": [
        "## Data Augmenttion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0vSiW0aVWoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Data Augmentation => taking the batch and apply some series of random transformations (random rotation, resizing, shearing) \n",
        "\n",
        "      ===> to increase generalizability of model  \"\"\"\n",
        "\n",
        "\n",
        "# data generator Generate batches of tensor image data with real-time data augmentation\n",
        "data_generator = ImageDataGenerator(\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIW7y7yWkwJ3",
        "colab_type": "text"
      },
      "source": [
        "**ImageDataGenerator**\n",
        "* horizontal and vertical shift => moving all pixels of image in one direction \n",
        "  1. width_shift_range ( horizonatal shift)\n",
        "  2. height_shift_range (vertical shift)\n",
        "  3. floating num [0- 1] --> % of shift\n",
        "\n",
        "* horizontal n vertical flips augmentation ==>reversing rows or cols of pixels  --> True or False\n",
        "* Random rotation  --> 0 - 360 degrees --> rotation_range = 90 ==> means random rotation to image blw 0 and 90 degrees\n",
        "* random  brightness --> randomly darkens or brightens images ==> brightness_range =[0.2,1.0] --> means darkens or brightens if pixel is blw 0.2 and 1\n",
        "* random zoom\n",
        "  1. either adds pixel or subtract pixels in image . [1-value, 1+value] \n",
        "  2. for example , zoom_range = .3 --> range [0.7, 1.3] or blw 70%(zoom in) and 130% (zoomout)\n",
        "\n",
        "---\n",
        "when an object is created using following args. an iterator can be created for an image dataset. \n",
        "* it iterates through all images in memory --> obj.flow(X,y)\n",
        "* to iterates images through subdirectories --> obj. flow_from_directory(X,y,..)\n",
        "* to train ==> fit_generator() "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeF9vG6oWMB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b68c3b98-06c1-46d3-951c-af8531bd75c1"
      },
      "source": [
        "# model parameters/compilation\n",
        "\n",
        "\"\"\" CONFIGURATION ==>.compile(optimizer, loss , metrics) \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image_array (Conv2D)         (None, 48, 48, 16)        800       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 16)        12560     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 48, 48, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 32)        12832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 12, 12, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 12, 12, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 6, 6, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_4 (Average (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 3, 3, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 3, 3, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 7)           16135     \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 7)                 0         \n",
            "_________________________________________________________________\n",
            "predictions (Activation)     (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 642,935\n",
            "Trainable params: 641,463\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZP8riBlO4mS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ab48817-fcb3-4507-df52-97b59527b165"
      },
      "source": [
        "datasets = ['fer2013']\n",
        "num_epochs = 30\n",
        "base_path=\"/content\"\n",
        "for dataset_name in datasets:\n",
        "    print('Training dataset:', dataset_name)\n",
        "\n",
        "    #callbacks\n",
        "    log_file_path = dataset_name + '_emotion_training.log'\n",
        "\n",
        "    csv_logger = CSVLogger(log_file_path, append=False)\n",
        "    early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,patience=int(patience/4), verbose=1)\n",
        "    \n",
        "    trained_models_path = base_path + dataset_name + 'simple_cnn'\n",
        "    model_names = trained_models_path + '.{epoch:02d}-{val_loss:.2f}.hdf5'      # if error \"acc\" in 1 line ... don't confuse check entire block since fit() generates a inner loop\n",
        "    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
        "    my_callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
        "\n",
        "    # loading dataset\n",
        "    train_faces, train_emotions = train_data\n",
        "    history=model.fit_generator(data_generator.flow(train_faces, train_emotions,\n",
        "                                            batch_size),\n",
        "                        epochs=num_epochs, verbose=1\n",
        "                        ,callbacks=my_callbacks,validation_data =val_data)   #not callbacks = [my_callbacks] since we my_callbacks is already a list "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset: fer2013\n",
            "Epoch 1/30\n",
            "898/898 [==============================] - 37s 41ms/step - loss: 1.7446 - accuracy: 0.3049 - val_loss: 1.6654 - val_accuracy: 0.3469\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.66542, saving model to /contentfer2013simple_cnn.01-1.67.hdf5\n",
            "Epoch 2/30\n",
            "898/898 [==============================] - 27s 30ms/step - loss: 1.6229 - accuracy: 0.3599 - val_loss: 1.5122 - val_accuracy: 0.4136\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.66542 to 1.51218, saving model to /contentfer2013simple_cnn.02-1.51.hdf5\n",
            "Epoch 3/30\n",
            "898/898 [==============================] - 27s 30ms/step - loss: 1.5591 - accuracy: 0.3882 - val_loss: 1.4288 - val_accuracy: 0.4473\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.51218 to 1.42880, saving model to /contentfer2013simple_cnn.03-1.43.hdf5\n",
            "Epoch 4/30\n",
            "898/898 [==============================] - 27s 30ms/step - loss: 1.5137 - accuracy: 0.4102 - val_loss: 1.3665 - val_accuracy: 0.4759\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.42880 to 1.36649, saving model to /contentfer2013simple_cnn.04-1.37.hdf5\n",
            "Epoch 5/30\n",
            "898/898 [==============================] - 27s 30ms/step - loss: 1.4730 - accuracy: 0.4316 - val_loss: 1.3454 - val_accuracy: 0.4852\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.36649 to 1.34536, saving model to /contentfer2013simple_cnn.05-1.35.hdf5\n",
            "Epoch 6/30\n",
            "898/898 [==============================] - 27s 30ms/step - loss: 1.4482 - accuracy: 0.4409 - val_loss: 1.3031 - val_accuracy: 0.5021\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.34536 to 1.30313, saving model to /contentfer2013simple_cnn.06-1.30.hdf5\n",
            "Epoch 7/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.4202 - accuracy: 0.4541 - val_loss: 1.3007 - val_accuracy: 0.5079\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.30313 to 1.30072, saving model to /contentfer2013simple_cnn.07-1.30.hdf5\n",
            "Epoch 8/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.4081 - accuracy: 0.4610 - val_loss: 1.2745 - val_accuracy: 0.5052\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.30072 to 1.27453, saving model to /contentfer2013simple_cnn.08-1.27.hdf5\n",
            "Epoch 9/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.3888 - accuracy: 0.4678 - val_loss: 1.2456 - val_accuracy: 0.5255\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.27453 to 1.24560, saving model to /contentfer2013simple_cnn.09-1.25.hdf5\n",
            "Epoch 10/30\n",
            "898/898 [==============================] - 27s 30ms/step - loss: 1.3702 - accuracy: 0.4759 - val_loss: 1.2763 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.24560\n",
            "Epoch 11/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.3630 - accuracy: 0.4778 - val_loss: 1.2710 - val_accuracy: 0.5162\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.24560\n",
            "Epoch 12/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.3511 - accuracy: 0.4833 - val_loss: 1.2702 - val_accuracy: 0.5181\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.24560\n",
            "Epoch 13/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.3365 - accuracy: 0.4922 - val_loss: 1.2123 - val_accuracy: 0.5337\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.24560 to 1.21234, saving model to /contentfer2013simple_cnn.13-1.21.hdf5\n",
            "Epoch 14/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.3364 - accuracy: 0.4879 - val_loss: 1.2081 - val_accuracy: 0.5422\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.21234 to 1.20806, saving model to /contentfer2013simple_cnn.14-1.21.hdf5\n",
            "Epoch 15/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.3291 - accuracy: 0.4931 - val_loss: 1.2064 - val_accuracy: 0.5357\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.20806 to 1.20638, saving model to /contentfer2013simple_cnn.15-1.21.hdf5\n",
            "Epoch 16/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.3167 - accuracy: 0.4967 - val_loss: 1.1628 - val_accuracy: 0.5606\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.20638 to 1.16279, saving model to /contentfer2013simple_cnn.16-1.16.hdf5\n",
            "Epoch 17/30\n",
            "898/898 [==============================] - 27s 30ms/step - loss: 1.3130 - accuracy: 0.4991 - val_loss: 1.1738 - val_accuracy: 0.5500\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.16279\n",
            "Epoch 18/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.3083 - accuracy: 0.5014 - val_loss: 1.1813 - val_accuracy: 0.5481\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.16279\n",
            "Epoch 19/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.2999 - accuracy: 0.5035 - val_loss: 1.2064 - val_accuracy: 0.5298\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.16279\n",
            "Epoch 20/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.3074 - accuracy: 0.5015 - val_loss: 1.1540 - val_accuracy: 0.5595\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.16279 to 1.15402, saving model to /contentfer2013simple_cnn.20-1.15.hdf5\n",
            "Epoch 21/30\n",
            "898/898 [==============================] - 26s 28ms/step - loss: 1.2921 - accuracy: 0.5103 - val_loss: 1.1651 - val_accuracy: 0.5535\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.15402\n",
            "Epoch 22/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.2921 - accuracy: 0.5087 - val_loss: 1.1362 - val_accuracy: 0.5690\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.15402 to 1.13617, saving model to /contentfer2013simple_cnn.22-1.14.hdf5\n",
            "Epoch 23/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.2882 - accuracy: 0.5093 - val_loss: 1.1399 - val_accuracy: 0.5663\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.13617\n",
            "Epoch 24/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.2854 - accuracy: 0.5111 - val_loss: 1.1724 - val_accuracy: 0.5563\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.13617\n",
            "Epoch 25/30\n",
            "898/898 [==============================] - 25s 28ms/step - loss: 1.2772 - accuracy: 0.5147 - val_loss: 1.1784 - val_accuracy: 0.5536\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.13617\n",
            "Epoch 26/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.2736 - accuracy: 0.5157 - val_loss: 1.1752 - val_accuracy: 0.5503\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.13617\n",
            "Epoch 27/30\n",
            "898/898 [==============================] - 27s 30ms/step - loss: 1.2792 - accuracy: 0.5131 - val_loss: 1.1507 - val_accuracy: 0.5575\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.13617\n",
            "Epoch 28/30\n",
            "898/898 [==============================] - 26s 28ms/step - loss: 1.2703 - accuracy: 0.5182 - val_loss: 1.1351 - val_accuracy: 0.5624\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.13617 to 1.13507, saving model to /contentfer2013simple_cnn.28-1.14.hdf5\n",
            "Epoch 29/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.2687 - accuracy: 0.5175 - val_loss: 1.1815 - val_accuracy: 0.5410\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.13507\n",
            "Epoch 30/30\n",
            "898/898 [==============================] - 26s 29ms/step - loss: 1.2706 - accuracy: 0.5166 - val_loss: 1.1163 - val_accuracy: 0.5697\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.13507 to 1.11626, saving model to /contentfer2013simple_cnn.30-1.12.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py6W0E8z6SId",
        "colab_type": "text"
      },
      "source": [
        "**keras**\n",
        "* **callbacks** => an object that can perform actions at various stages of training\n",
        "      1. write tensorflowboard logs after every batch\n",
        "      2. periodically save model to disk\n",
        "      3. do early stopping \n",
        "      4. view on internal states and statistics during training\n",
        "      * used in fit() loop\n",
        "  * **CSVLogger(filename, separator=\",')**  --> to save epoch results to a csv file\n",
        "      * create obj and use that obj in fit(callbacks=[csv_logger_obj])\n",
        "  * **EarlyStopping()** --> stop training when a monitored metric \n",
        "  has stopped improving\n",
        "      1. monitor = \"val_loss\" --> loss function to be monitored\n",
        "      2. min_delta --> minimum change to count(threshold)\n",
        "      3. patience --> no of epochs with no improvement to stop training\n",
        "  * **ReduceLROnPlateau()**--> reduce learning rate when metric has stopped improving\n",
        "      1. monitor, patience, min_delta\n",
        "      2. factor = 0.1 ==> learning rate reduced to 10% (lr*0.1)\n",
        "      3. verbose ==> 0: quiet , 1: update msgs\n",
        "  * **ModelCheckpoint()** -->to save  keras model or model weights at some frequency\n",
        "      1. filepath\n",
        "      2. monitor --> val_acc or val_loss\n",
        "      3. save_best_only = True \n",
        "\n",
        "---\n",
        "fit_generator(.flow(X,y, batchsize), verbose,epochs,validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aTQpoDjDdsU",
        "colab_type": "text"
      },
      "source": [
        "* fit() ==> training loop\n",
        "* logs ==> dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI4nPHW1WZ4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "13b47143-8b6f-439f-c47c-49586042f5e3"
      },
      "source": [
   